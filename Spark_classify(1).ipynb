{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark import rdd\n",
    "conf=SparkConf().setMaster(\"local[*]\").setAppName(\"First_APP\")\n",
    "sc=SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------\n",
    "2 从数据中抽取合适的特征\n",
    "----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "MLlib中的分类模型通过LabeledPoint对象操作,其中封装了目标变量(标签)和特征向量:\n",
    "\n",
    "                LabeledPoint(label: Double, features: Vector) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "虽然在使用分类模型的很多样例中会碰到向量格式的数据集,但在实际工作中,通常还需要从原始数据中抽取特征 ，包括封装数值特征、归一或者正则化特征,以及使用1-of-k编码表示类属特征 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"http://www.bloomberg.com/news/2010-12-23/ibm-predicts-holographic-calls-air-breathing-batteries-by-2015.html\"',\n",
       " '\"4042\"',\n",
       " '\"{\"\"title\"\":\"\"IBM Sees Holographic Calls Air Breathing Batteries ibm sees holographic calls, air-breathing batteries\"\",\"\"body\"\":\"\"A sign stands outside the International Business Machines Corp IBM Almaden Research Center campus in San Jose California Photographer Tony Avelar Bloomberg Buildings stand at the International Business Machines Corp IBM Almaden Research Center campus in the Santa Teresa Hills of San Jose California Photographer Tony Avelar Bloomberg By 2015 your mobile phone will project a 3 D image of anyone who calls and your laptop will be powered by kinetic energy At least that s what International Business Machines Corp sees in its crystal ball The predictions are part of an annual tradition for the Armonk New York based company which surveys its 3 000 researchers to find five ideas expected to take root in the next five years IBM the world s largest provider of computer services looks to Silicon Valley for input gleaning many ideas from its Almaden research center in San Jose California Holographic conversations projected from mobile phones lead this year s list The predictions also include air breathing batteries computer programs that can tell when and where traffic jams will take place environmental information generated by sensors in cars and phones and cities powered by the heat thrown off by computer servers These are all stretch goals and that s good said Paul Saffo managing director of foresight at the investment advisory firm Discern in San Francisco In an era when pessimism is the new black a little dose of technological optimism is not a bad thing For IBM it s not just idle speculation The company is one of the few big corporations investing in long range research projects and it counts on innovation to fuel growth Saffo said Not all of its predictions pan out though IBM was overly optimistic about the spread of speech technology for instance When the ideas do lead to products they can have broad implications for society as well as IBM s bottom line he said Research Spending They have continued to do research when all the other grand research organizations are gone said Saffo who is also a consulting associate professor at Stanford University IBM invested 5 8 billion in research and development last year 6 1 percent of revenue While that s down from about 10 percent in the early 1990s the company spends a bigger share on research than its computing rivals Hewlett Packard Co the top maker of personal computers spent 2 4 percent last year At Almaden scientists work on projects that don t always fit in with IBM s computer business The lab s research includes efforts to develop an electric car battery that runs 500 miles on one charge a filtration system for desalination and a program that shows changes in geographic data IBM rose 9 cents to 146 04 at 11 02 a m in New York Stock Exchange composite trading The stock had gained 11 percent this year before today Citizen Science The list is meant to give a window into the company s innovation engine said Josephine Cheng a vice president at IBM s Almaden lab All this demonstrates a real culture of innovation at IBM and willingness to devote itself to solving some of the world s biggest problems she said Many of the predictions are based on projects that IBM has in the works One of this year s ideas that sensors in cars wallets and personal devices will give scientists better data about the environment is an expansion of the company s citizen science initiative Earlier this year IBM teamed up with the California State Water Resources Control Board and the City of San Jose Environmental Services to help gather information about waterways Researchers from Almaden created an application that lets smartphone users snap photos of streams and creeks and report back on conditions The hope is that these casual observations will help local and state officials who don t have the resources to do the work themselves Traffic Predictors IBM also sees data helping shorten commutes in the next five years Computer programs will use algorithms and real time traffic information to predict which roads will have backups and how to avoid getting stuck Batteries may last 10 times longer in 2015 than today IBM says Rather than using the current lithium ion technology new models could rely on energy dense metals that only need to interact with the air to recharge Some electronic devices might ditch batteries altogether and use something similar to kinetic wristwatches which only need to be shaken to generate a charge The final prediction involves recycling the heat generated by computers and data centers Almost half of the power used by data centers is currently spent keeping the computers cool IBM scientists say it would be better to harness that heat to warm houses and offices In IBM s first list of predictions compiled at the end of 2006 researchers said instantaneous speech translation would become the norm That hasn t happened yet While some programs can quickly translate electronic documents and instant messages and other apps can perform limited speech translation there s nothing widely available that acts like the universal translator in Star Trek Second Life The company also predicted that online immersive environments such as Second Life would become more widespread While immersive video games are as popular as ever Second Life s growth has slowed Internet users are flocking instead to the more 2 D environments of Facebook Inc and Twitter Inc Meanwhile a 2007 prediction that mobile phones will act as a wallet ticket broker concierge bank and shopping assistant is coming true thanks to the explosion of smartphone applications Consumers can pay bills through their banking apps buy movie tickets and get instant feedback on potential purchases all with a few taps on their phones The nice thing about the list is that it provokes thought Saffo said If everything came true they wouldn t be doing their job To contact the reporter on this story Ryan Flinn in San Francisco at rflinn bloomberg net To contact the editor responsible for this story Tom Giles at tgiles5 bloomberg net by 2015, your mobile phone will project a 3-d image of anyone who calls and your laptop will be powered by kinetic energy. at least that\\\\u2019s what international business machines corp. sees in its crystal ball.\"\",\"\"url\"\":\"\"bloomberg news 2010 12 23 ibm predicts holographic calls air breathing batteries by 2015 html\"\"}\"',\n",
       " '\"business\"',\n",
       " '\"0.789131\"',\n",
       " '\"2.055555556\"',\n",
       " '\"0.676470588\"',\n",
       " '\"0.205882353\"',\n",
       " '\"0.047058824\"',\n",
       " '\"0.023529412\"',\n",
       " '\"0.443783175\"',\n",
       " '\"0\"',\n",
       " '\"0\"',\n",
       " '\"0.09077381\"',\n",
       " '\"0\"',\n",
       " '\"0.245831182\"',\n",
       " '\"0.003883495\"',\n",
       " '\"1\"',\n",
       " '\"1\"',\n",
       " '\"24\"',\n",
       " '\"0\"',\n",
       " '\"5424\"',\n",
       " '\"170\"',\n",
       " '\"8\"',\n",
       " '\"0.152941176\"',\n",
       " '\"0.079129575\"',\n",
       " '\"0\"']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawData = sc.textFile('E:/machine_data/StumbleUpon evergreen/train.tsv')\n",
    "records = rawData.map(lambda x: x.split('\\t'))\n",
    "records=sc.parallelize(records.collect()[1:])\n",
    "records.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('0', ['0.789131', '2.055555556', '0.676470588', '0.205882353', '0.047058824', '0.023529412', '0.443783175', '0', '0', '0.09077381', '0', '0.245831182', '0.003883495', '1', '1', '24', '0', '5424', '170', '8', '0.152941176', '0.079129575'])\n",
      "('0', ['0.789131', '2.055555556', '0.676470588', '0.205882353', '0.047058824', '0.023529412', '0.443783175', '0', '0', '0.09077381', '0', '0.245831182', '0.003883495', '1', '1', '24', '0', '5424', '170', '8', '0.152941176', '0.079129575'])\n",
      "('0', ['0.789131', '2.055555556', '0.676470588', '0.205882353', '0.047058824', '0.023529412', '0.443783175', '0', '0', '0.09077381', '0', '0.245831182', '0.003883495', '1', '1', '24', '0', '5424', '170', '8', '0.152941176', '0.079129575'])\n",
      "(0, [0.789131, 2.055555556, 0.676470588, 0.205882353, 0.047058824, 0.023529412, 0.443783175, 0.0, 0.0, 0.09077381, 0.0, 0.245831182, 0.003883495, 1.0, 1.0, 24.0, 0.0, 5424.0, 170.0, 8.0, 0.152941176, 0.079129575])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(0.0, [0.789131,2.055555556,0.676470588,0.205882353,0.047058824,0.023529412,0.443783175,0.0,0.0,0.09077381,0.0,0.245831182,0.003883495,1.0,1.0,24.0,0.0,5424.0,170.0,8.0,0.152941176,0.079129575]),\n",
       " LabeledPoint(1.0, [0.574147,3.677966102,0.50802139,0.288770053,0.213903743,0.144385027,0.468648998,0.0,0.0,0.098707403,0.0,0.203489628,0.088652482,1.0,1.0,40.0,0.0,4973.0,187.0,9.0,0.181818182,0.125448029]),\n",
       " LabeledPoint(1.0, [0.996526,2.382882883,0.562015504,0.321705426,0.120155039,0.042635659,0.525448029,0.0,0.0,0.072447859,0.0,0.22640177,0.120535714,1.0,1.0,55.0,0.0,2240.0,258.0,11.0,0.166666667,0.057613169]),\n",
       " LabeledPoint(1.0, [0.801248,1.543103448,0.4,0.1,0.016666667,0.0,0.480724749,0.0,0.0,0.095860566,0.0,0.265655744,0.035343035,1.0,0.0,24.0,0.0,2737.0,120.0,5.0,0.041666667,0.100858369]),\n",
       " LabeledPoint(0.0, [0.719157,2.676470588,0.5,0.222222222,0.12345679,0.043209877,0.446143274,0.0,0.0,0.024908425,0.0,0.228887247,0.050473186,1.0,1.0,14.0,0.0,12032.0,162.0,10.0,0.098765432,0.082568807])]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "trimmed = records.map(lambda x: [xx.replace('\\\"','') for xx in x])\n",
    "label = trimmed.map(lambda x : x[-1])\n",
    "\n",
    "data1 = trimmed.map(lambda x : (x[-1],x[4:-1]))\n",
    "print (data1.first())\n",
    "data2=data1.map(lambda kv:(kv[0].replace('\\\"',''),[0.0 if yy=='\\\"?\\\"' else yy.replace('\\\"','') for yy in kv[1]]))\n",
    "print (data2.first())\n",
    "data3=data2.map(lambda kv:(kv[0].replace(\"\\\"\",\"\"),[0.0 if yy =='?' else yy.replace(\"\\\"\",\"\") for yy in kv[1]]))\n",
    "print (data3.first())\n",
    "data4=data3.map(lambda kv:(int(kv[0]), [float(yy) for yy in kv[1]]))\n",
    "print (data4.first())\n",
    "data=data4.map(lambda kv:LabeledPoint(kv[0],Vectors.dense(kv[1])))\n",
    "data.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1==data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7395\n"
     ]
    }
   ],
   "source": [
    "#对数据进行缓存，同时统计数据样本的数目：\n",
    "data.cache()\n",
    "numData = data.count()\n",
    "print (numData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在对数据集做进一步处理之前,我们发现数值数据中包含负的特征值。我们知道,朴素贝叶斯模型要求特征值非负,否则碰到负的特征值程序会抛出错误。因此,需要为朴素贝叶斯模型构建一份输入特征向量的数据,将负特征值设为0: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(0.0, [0.789131,2.055555556,0.676470588,0.205882353,0.047058824,0.023529412,0.443783175,0.0,0.0,0.09077381,0.0,0.245831182,0.003883495,1.0,1.0,24.0,0.0,5424.0,170.0,8.0,0.152941176,0.079129575]),\n",
       " LabeledPoint(1.0, [0.574147,3.677966102,0.50802139,0.288770053,0.213903743,0.144385027,0.468648998,0.0,0.0,0.098707403,0.0,0.203489628,0.088652482,1.0,1.0,40.0,0.0,4973.0,187.0,9.0,0.181818182,0.125448029])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbdata = trimmed.map(lambda x : (x[-1],x[4:-1]))\n",
    "nbdata1=nbdata.map(lambda kv:(kv[0].replace('\\\"',''),[0.0 if yy=='\\\"?\\\"' else yy.replace('\\\"','') for yy in kv[1]]))\n",
    "nbdata2=nbdata1.map(lambda kv:(kv[0].replace(\"\\\"\",\"\"),[0.0 if yy =='?' else yy.replace(\"\\\"\",\"\") for yy in kv[1]]))\n",
    "nbdata3=nbdata2.map(lambda kv:(int(kv[0]), [float(yy) for yy in kv[1]]))\n",
    "nbdata4=nbdata3.map(lambda kv:(kv[0], [0.0 if yy<0 else yy for yy in kv[1]]))\n",
    "nbdata =nbdata4.map(lambda kv:LabeledPoint(kv[0],Vectors.dense(kv[1])))\n",
    "nbdata.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "类别数： 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda1\\lib\\site-packages\\pyspark\\mllib\\classification.py:313: UserWarning: Deprecated in 2.0.0. Use ml.classification.LogisticRegression or LogisticRegressionWithLBFGS.\n",
      "  \"Deprecated in 2.0.0. Use ml.classification.LogisticRegression or \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "逻辑回归模型参数： (weights=[-0.110216274454,-0.493200344739,-0.0712665620384,-0.0214744216778,0.00276706475384,0.00246385887598,-1.33300460292,0.0525232672351,0.0,-0.0320576776,-0.00653638798541,-0.0613702511674,-0.14975863133,-0.13648187383,-0.121161700009,-15.6451616669,-0.0177690355464,745.987958686,-7.73567729685,-1.38587998188,-0.0355600416613,-0.0352085128613], intercept=0.0)\n",
      "支持向量机模型参数： (weights=[-0.122188386978,-0.527510758159,-0.0742371782434,-0.0206667449306,0.00546395033577,0.00409811283781,-1.54824523474,0.0607028905087,0.0,-0.037008323802,-0.007374037142,-0.067970375864,-0.172289581054,-0.148716595522,-0.129369384966,-18.0315472516,-0.0202704220321,1025.48043141,-5.05188911633,-1.54111193167,-0.038689478606,-0.0397619278886], intercept=0.0)\n",
      "朴素贝叶斯模型参数： <pyspark.mllib.classification.NaiveBayesModel object at 0x000002753E7F17B8>\n",
      "决策树模型参数： DecisionTreeModel classifier of depth 5 with 61 nodes\n"
     ]
    }
   ],
   "source": [
    "#导入相应的类\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "from pyspark.mllib.classification import SVMWithSGD\n",
    "from pyspark.mllib.classification import NaiveBayes\n",
    "from pyspark.mllib.tree import DecisionTree\n",
    "\n",
    "numIteration = 10   #迭代次数\n",
    "maxTreeDepth = 5    #树的深度\n",
    "\n",
    "numClass = label.distinct().count()     #类别数\n",
    "print ('类别数：',numClass)\n",
    "\n",
    "#训练逻辑回归、支持向量机、朴素贝叶斯和决策树模型\n",
    "lrModel = LogisticRegressionWithSGD.train(data, numIteration)\n",
    "svmModel = SVMWithSGD.train(data, numIteration)\n",
    "nbModel = NaiveBayes.train(nbdata)\n",
    "dtModel = DecisionTree.trainClassifier(data,numClass,{},impurity='entropy', maxDepth=maxTreeDepth)\n",
    "print ('逻辑回归模型参数：',lrModel)\n",
    "print ('支持向量机模型参数：',svmModel)\n",
    "print ('朴素贝叶斯模型参数：',nbModel)\n",
    "print ('决策树模型参数：',dtModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------\n",
    "4 使用分类模型 \n",
    "--------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测的类别： 1\n",
      "真实的类别： 0\n"
     ]
    }
   ],
   "source": [
    "dataPoint = data.first()\n",
    "prediction = lrModel.predict(dataPoint.features)\n",
    "print ('预测的类别：',prediction)\n",
    "print ('真实的类别：',int(dataPoint.label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "前十个样本的预测标签： [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "前十个样本的真实标签： [0, 1, 1, 1, 0, 0, 1, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "predictions = lrModel.predict(data.map(lambda x : x.features))\n",
    "print ('前十个样本的预测标签：',predictions.take(10))\n",
    "print ('前十个样本的真实标签：',data.map(lambda x : x.label).map(lambda x: int(x)).take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------\n",
    "5 评估分类模型的性能 \n",
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------\n",
    "5.1 预测的正确率和错误率\n",
    "-----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总共的样本数目: 7395\n",
      "逻辑回归模型模型的预测正确率: 0.5146720757268425\n",
      "支持向量机模型的预测正确率: 0.514672\n",
      "朴素贝叶斯模型的预测正确率: 0.580392\n",
      "决策树模型的预测正确率: 0.648276\n"
     ]
    }
   ],
   "source": [
    "#逻辑回归模型模型的预测正确数目\n",
    "lrTotalCorrect = data.map(lambda point : 1 if(lrModel.predict(point.features)==point.label) else 0).sum()\n",
    "#支持向量机模型的预测正确数目\n",
    "svmTotalCorrect = data.map(lambda point : 1 if(svmModel.predict(point.features)==point.label) else 0).sum()\n",
    "#朴素贝叶斯模型的预测正确数目\n",
    "nbTotalCorrect = nbdata.map(lambda point : 1 if (nbModel.predict(point.features) == point.label) else 0).sum()\n",
    "\n",
    "#决策树模型的预测正确数目\n",
    "predictLabel= dtModel.predict(data.map(lambda point: point.features)).collect()\n",
    "trueLabel = data.map(lambda point: point.label).collect()\n",
    "dtTotalCorrect = sum([1.0 if prediction == trueLabel[i] else 0.0 for i, prediction in enumerate(predictLabel)])\n",
    "\n",
    "#逻辑回归模型模型的预测正确率\n",
    "lrAccuracy = lrTotalCorrect/(data.count()*1.0)\n",
    "#支持向量机模型的预测正确率\n",
    "svmAccuracy = svmTotalCorrect/(data.count()*1.0)\n",
    "#朴素贝叶斯模型的预测正确率\n",
    "nbAccuracy = nbTotalCorrect/(1.0*nbdata.count())\n",
    "#决策树模型的预测正确率\n",
    "dtAccuracy = dtTotalCorrect/(1.0*data.count())\n",
    "print ('总共的样本数目: %s'%data.count())\n",
    "print ('逻辑回归模型模型的预测正确率: %s'%lrAccuracy)\n",
    "print ('支持向量机模型的预测正确率: %f'%svmAccuracy)\n",
    "print ('朴素贝叶斯模型的预测正确率: %f'%nbAccuracy)\n",
    "print ('决策树模型的预测正确率: %f'%dtAccuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------\n",
    "5.2 准确率和召回率 \n",
    "-------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegressionModel的AUC是0.501418,PR是0.756759\n",
      "SVMModel的AUC是0.501418,PR是0.756759\n"
     ]
    }
   ],
   "source": [
    "#MLlib内置了一系列方法用来计算二分类的PR和ROC曲线下的面积。\n",
    "# 模型评价\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "all_models_metrics = []\n",
    "for model in [lrModel, svmModel]:\n",
    "    scoresAndLabels = data.map(lambda point:(model.predict(point.features), point.label)).collect()\n",
    "    scoresAndLabels = [[float(i),j] for i,j in scoresAndLabels]\n",
    "    \n",
    "    rdd_scoresAndLabels = sc.parallelize(scoresAndLabels)   #将数据转换为rdd\n",
    "    metrics = BinaryClassificationMetrics(rdd_scoresAndLabels)\n",
    "    all_models_metrics.append((model.__class__.__name__, metrics.areaUnderROC, metrics.areaUnderPR))\n",
    "\n",
    "for modelName, AUC, PR in all_models_metrics:\n",
    "    print ('%s的AUC是%f,PR是%f'%(modelName, AUC, PR))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeModel的AUC是0.648837,PR是0.743081\n"
     ]
    }
   ],
   "source": [
    "#之前已经训练朴素贝叶斯模型并计算准确率,其中使用的数据集是nbData,这里使用同样的数据集进行模型评价。\n",
    "predictionLabels = dtModel.predict(data.map(lambda point: point.features)).collect()\n",
    "trueLabels = data.map(lambda point: point.label).collect()\n",
    "scoresAndLabels = [[prediction, trueLabel] for prediction,trueLabel in zip(predictionLabels, trueLabels)]\n",
    "scoresAndLabels = [[float(i),j] for i,j in scoresAndLabels]\n",
    "rdd_scoresAndLabels = sc.parallelize(scoresAndLabels)\n",
    "\n",
    "dt_metric = BinaryClassificationMetrics(rdd_scoresAndLabels)\n",
    "print ('%s的AUC是%f,PR是%f'%(dtModel.__class__.__name__, dt_metric.areaUnderROC, dt_metric.areaUnderPR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------\n",
    "6 改进模型性能以及参数调优 \n",
    "-------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "6.1 特征标准化 \n",
    "----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method JavaModelWrapper.__del__ of <pyspark.mllib.stat._statistics.MultivariateStatisticalSummary object at 0x000002753E75D198>>\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\anaconda1\\lib\\site-packages\\pyspark\\mllib\\common.py\", line 142, in __del__\n",
      "    self._sc._gateway.detach(self._java_model)\n",
      "  File \"D:\\anaconda1\\lib\\site-packages\\py4j\\java_gateway.py\", line 1870, in detach\n",
      "    java_object._detach()\n",
      "AttributeError: 'PipelinedRDD' object has no attribute '_detach'\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 12.0 failed 1 times, most recent failure: Lost task 0.0 in stage 12.0 (TID 28, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"E:\\Spark\\spark-2.1.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 174, in main\n  File \"E:\\Spark\\spark-2.1.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 169, in process\n  File \"E:\\Spark\\spark-2.1.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"D:\\anaconda1\\lib\\site-packages\\pyspark\\rdd.py\", line 833, in func\n    yield reduce(f, iterator, initial)\nTypeError: unorderable types: DenseVector() < DenseVector()\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1958)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:935)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:934)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:453)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"E:\\Spark\\spark-2.1.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 174, in main\n  File \"E:\\Spark\\spark-2.1.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 169, in process\n  File \"E:\\Spark\\spark-2.1.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"D:\\anaconda1\\lib\\site-packages\\pyspark\\rdd.py\", line 833, in func\n    yield reduce(f, iterator, initial)\nTypeError: unorderable types: DenseVector() < DenseVector()\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-a18667afe38a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mmatrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMultivariateStatisticalSummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mmatrix_mean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mmatrix_min\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMultivariateStatisticalSummary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'数据集每列的均值:'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmatrix_mean\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'数据集每列的最小值:'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmatrix_min\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda1\\lib\\site-packages\\pyspark\\mllib\\stat\\_statistics.py\u001b[0m in \u001b[0;36mmin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"min\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoArray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mnormL1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda1\\lib\\site-packages\\pyspark\\mllib\\common.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, name, *a)\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[1;34m\"\"\"Call method of java_model\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcallJavaFunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda1\\lib\\site-packages\\pyspark\\mllib\\common.py\u001b[0m in \u001b[0;36mcallJavaFunc\u001b[1;34m(sc, func, *args)\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[1;34m\"\"\" Call Java Function \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_py2java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_java2py\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda1\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mmin\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1020\u001b[0m         \"\"\"\n\u001b[0;32m   1021\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1022\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1023\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda1\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mreduce\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    833\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    834\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 835\u001b[1;33m         \u001b[0mvals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    836\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvals\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    837\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda1\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    807\u001b[0m         \"\"\"\n\u001b[0;32m    808\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 809\u001b[1;33m             \u001b[0mport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    810\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    811\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda1\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1133\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda1\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    318\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    320\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 12.0 failed 1 times, most recent failure: Lost task 0.0 in stage 12.0 (TID 28, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"E:\\Spark\\spark-2.1.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 174, in main\n  File \"E:\\Spark\\spark-2.1.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 169, in process\n  File \"E:\\Spark\\spark-2.1.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"D:\\anaconda1\\lib\\site-packages\\pyspark\\rdd.py\", line 833, in func\n    yield reduce(f, iterator, initial)\nTypeError: unorderable types: DenseVector() < DenseVector()\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1958)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:935)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:934)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:453)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"E:\\Spark\\spark-2.1.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 174, in main\n  File \"E:\\Spark\\spark-2.1.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 169, in process\n  File \"E:\\Spark\\spark-2.1.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"D:\\anaconda1\\lib\\site-packages\\pyspark\\rdd.py\", line 833, in func\n    yield reduce(f, iterator, initial)\nTypeError: unorderable types: DenseVector() < DenseVector()\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.mllib.stat import MultivariateStatisticalSummary\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "\n",
    "vectors = data.map(lambda point : point.features)\n",
    "matrix = MultivariateStatisticalSummary(vectors)\n",
    "matrix_mean = matrix.mean()\n",
    "matrix_min = matrix.min()\n",
    "print ('数据集每列的均值:',matrix_mean)\n",
    "print ('数据集每列的最小值:',matrix_min)\n",
    "print ('数据集每列的最大值:',matrix.max())\n",
    "print ('数据集的样本数:',matrix.count())\n",
    "print ('数据集每列的方差:',matrix.variance())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "观察前面对均值和方差的输出,可以清晰发现第二个特征的方差和均值比其他的都要高(你会发现一些其他特征也有类似的结果,而且有些特征更加极端)。因为我们的数据在原始形式下,确切地说并不符合标准的高斯分布。为使数据更符合模型的假设,可以对每个特征进行标准化,使得每个特征是0均值和单位标准差。具体做法是对每个特征值减去列的均值,然后除以列的标准差以进行缩放:                  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                                              (x–μ) / sqrt(variance) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "使用Spark的StandardScaler中的方法方便地完成这个任务。 首先，传入两个参数,一个表示是否从数据中减去均值,另一个表示是否应用标准差缩放。这样使得StandardScaler和我们的输入向量相符。最后,将输入向量传到转换函数,并且返回归一化的向量。具体实现代码如下,我们使用map函数来保留数据集的标签: \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始数据: [DenseVector([0.7891, 2.0556, 0.6765, 0.2059, 0.0471, 0.0235, 0.4438, 0.0, 0.0, 0.0908, 0.0, 0.2458, 0.0039, 1.0, 1.0, 24.0, 0.0, 5424.0, 170.0, 8.0, 0.1529, 0.0791]), DenseVector([0.5741, 3.678, 0.508, 0.2888, 0.2139, 0.1444, 0.4686, 0.0, 0.0, 0.0987, 0.0, 0.2035, 0.0887, 1.0, 1.0, 40.0, 0.0, 4973.0, 187.0, 9.0, 0.1818, 0.1254]), DenseVector([0.9965, 2.3829, 0.562, 0.3217, 0.1202, 0.0426, 0.5254, 0.0, 0.0, 0.0724, 0.0, 0.2264, 0.1205, 1.0, 1.0, 55.0, 0.0, 2240.0, 258.0, 11.0, 0.1667, 0.0576])]\n",
      "规范化之后的数据: [DenseVector([1.1376, -0.0819, 1.0251, -0.0559, -0.4689, -0.3543, -0.3175, 0.3385, 0.0, 0.8288, -0.1473, 0.2296, -0.1416, 0.7902, 0.7172, -0.298, -0.2035, -0.033, -0.0488, 0.9401, -0.1087, -0.2788]), DenseVector([0.4887, 0.1063, 0.1959, 0.509, 1.2695, 1.3097, -0.3132, 0.3385, 0.0, 1.0202, -0.1473, -0.5771, -0.0975, 0.7902, 0.7172, 0.4866, -0.2035, -0.0838, 0.0459, 1.2494, 0.0489, 0.3058]), DenseVector([1.7637, -0.044, 0.4617, 0.7334, 0.2927, -0.0912, -0.3032, 0.3385, 0.0, 0.3867, -0.1473, -0.1405, -0.0808, 0.7902, 0.7172, 1.2221, -0.2035, -0.3917, 0.4416, 1.868, -0.0338, -0.5504])]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.feature import StandardScalerModel,StandardScaler\n",
    "\n",
    "scaler = StandardScaler(withMean=True, withStd=True).fit(vectors)\n",
    "labels = data.map(lambda point: point.label)\n",
    "features = data.map(lambda point: point.features)\n",
    "print ('原始数据:',features.take(3))\n",
    "scaled_data = labels.zip(scaler.transform(features))\n",
    "scaled_data = scaled_data.map(lambda kv: LabeledPoint(kv[0],kv[1]))\n",
    "print ('规范化之后的数据:',scaled_data.map(lambda point: point.features).take(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda1\\lib\\site-packages\\pyspark\\mllib\\classification.py:313: UserWarning: Deprecated in 2.0.0. Use ml.classification.LogisticRegression or LogisticRegressionWithLBFGS.\n",
      "  \"Deprecated in 2.0.0. Use ml.classification.LogisticRegression or \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征规范化之后逻辑回归预测的正确率: 0.6209601081812035\n"
     ]
    }
   ],
   "source": [
    "#现在我们使用标准化的数据重新训练模型。这里只训练逻辑回归(因为决策树和朴素贝叶斯不受特征标准化的影响),并说明特征标准化的影响: \n",
    "lrModel_scaled = LogisticRegressionWithSGD.train(scaled_data,numIteration)\n",
    "lrPredictionLabel = lrModel_scaled.predict(scaled_data.map(lambda point: point.features))\n",
    "trueLabel = scaled_data.map(lambda point: point.label)\n",
    "lrTotalCorrect_scaled = sum([1.0 if prediction==label else 0.0 \n",
    "                             for prediction, label in zip(lrPredictionLabel.collect(),trueLabel.collect())])/scaled_data.count()\n",
    "print ('特征规范化之后逻辑回归预测的正确率:',lrTotalCorrect_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegressionModel的AUC是0.620190,PR是0.727701\n"
     ]
    }
   ],
   "source": [
    "all_models_metrics =[]\n",
    "for model in [lrModel_scaled]:\n",
    "    scoresAndLabels = scaled_data.map(lambda point:(model.predict(point.features),point.label)).collect()\n",
    "    scoresAndLabels = [(float(i),j) for (i,j) in scoresAndLabels]\n",
    "    scoresAndLabels_rdd = sc.parallelize(scoresAndLabels)\n",
    "    metrics = BinaryClassificationMetrics(scoresAndLabels_rdd)\n",
    "    all_models_metrics.append((model.__class__.__name__,metrics.areaUnderROC, metrics.areaUnderPR))\n",
    "for model_name, AUC, PR in all_models_metrics:\n",
    "    print ('%s的AUC是%f,PR是%f'%(model_name, AUC, PR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "6.2 其他特征 \n",
    "---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('\"recreation\"', 0), ('\"culture_politics\"', 1), ('\"law_crime\"', 2), ('\"weather\"', 3), ('\"religion\"', 4), ('\"computer_internet\"', 5), ('\"science_technology\"', 6), ('\"unknown\"', 7), ('\"health\"', 8), ('\"sports\"', 9), ('\"?\"', 10), ('\"arts_entertainment\"', 11), ('\"gaming\"', 12), ('\"business\"', 13)]\n",
      "类别的编码: {'sports': 9, 'religion': 4, 'business': 13, 'culture_politics': 1, 'unknown': 7, 'arts_entertainment': 11, 'recreation': 0, 'law_crime': 2, 'health': 8, '?': 10, 'weather': 3, 'science_technology': 6, 'gaming': 12, 'computer_internet': 5}\n",
      "类别数: 14\n"
     ]
    }
   ],
   "source": [
    "category_dict = {}\n",
    "categories = records.map(lambda x: x[3]).distinct().zipWithIndex().collect()\n",
    "print (categories)\n",
    "for  (x,y) in [(key.replace('\\\"','') ,val) for (key, val) in categories]:\n",
    "    category_dict[x] = y\n",
    "\n",
    "print ('类别的编码:',category_dict)\n",
    "\n",
    "num_categories = len(category_dict)\n",
    "print ('类别数:',num_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(0.0, [0.789131,2.055555556,0.676470588,0.205882353,0.047058824,0.023529412,0.443783175,0.0,0.0,0.09077381,0.0,0.245831182,0.003883495,1.0,1.0,24.0,0.0,5424.0,170.0,8.0,0.152941176,0.079129575]),\n",
       " LabeledPoint(1.0, [0.574147,3.677966102,0.50802139,0.288770053,0.213903743,0.144385027,0.468648998,0.0,0.0,0.098707403,0.0,0.203489628,0.088652482,1.0,1.0,40.0,0.0,4973.0,187.0,9.0,0.181818182,0.125448029]),\n",
       " LabeledPoint(1.0, [0.996526,2.382882883,0.562015504,0.321705426,0.120155039,0.042635659,0.525448029,0.0,0.0,0.072447859,0.0,0.22640177,0.120535714,1.0,1.0,55.0,0.0,2240.0,258.0,11.0,0.166666667,0.057613169]),\n",
       " LabeledPoint(1.0, [0.801248,1.543103448,0.4,0.1,0.016666667,0.0,0.480724749,0.0,0.0,0.095860566,0.0,0.265655744,0.035343035,1.0,0.0,24.0,0.0,2737.0,120.0,5.0,0.041666667,0.100858369]),\n",
       " LabeledPoint(0.0, [0.719157,2.676470588,0.5,0.222222222,0.12345679,0.043209877,0.446143274,0.0,0.0,0.024908425,0.0,0.228887247,0.050473186,1.0,1.0,14.0,0.0,12032.0,162.0,10.0,0.098765432,0.082568807])]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "otherdata = trimmed.map(lambda x : (x[-1],x[4:-1]))\n",
    "otherdata1=otherdata.map(lambda kv:(kv[0].replace('\\\"',''),[0.0 if yy=='\\\"?\\\"' else yy.replace('\\\"','') for yy in kv[1]]))\n",
    "otherdata2=otherdata1.map(lambda kv:(kv[0].replace(\"\\\"\",\"\"),[0.0 if yy =='?' else yy.replace(\"\\\"\",\"\") for yy in kv[1]]))\n",
    "otherdata3=otherdata2.map(lambda kv:(int(kv[0]), [float(yy) for yy in kv[1]]))\n",
    "otherdata=otherdata3.map(lambda kv:LabeledPoint(kv[0],Vectors.dense(kv[1])))\n",
    "otherdata.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(0.0, [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.789131,2.055555556,0.676470588,0.205882353,0.047058824,0.023529412,0.443783175,0.0,0.0,0.09077381,0.0,0.245831182,0.003883495,1.0,1.0,24.0,0.0,5424.0,170.0,8.0,0.152941176,0.079129575]),\n",
       " LabeledPoint(1.0, [1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.574147,3.677966102,0.50802139,0.288770053,0.213903743,0.144385027,0.468648998,0.0,0.0,0.098707403,0.0,0.203489628,0.088652482,1.0,1.0,40.0,0.0,4973.0,187.0,9.0,0.181818182,0.125448029]),\n",
       " LabeledPoint(1.0, [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.996526,2.382882883,0.562015504,0.321705426,0.120155039,0.042635659,0.525448029,0.0,0.0,0.072447859,0.0,0.22640177,0.120535714,1.0,1.0,55.0,0.0,2240.0,258.0,11.0,0.166666667,0.057613169]),\n",
       " LabeledPoint(1.0, [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.801248,1.543103448,0.4,0.1,0.016666667,0.0,0.480724749,0.0,0.0,0.095860566,0.0,0.265655744,0.035343035,1.0,0.0,24.0,0.0,2737.0,120.0,5.0,0.041666667,0.100858369]),\n",
       " LabeledPoint(0.0, [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.719157,2.676470588,0.5,0.222222222,0.12345679,0.043209877,0.446143274,0.0,0.0,0.024908425,0.0,0.228887247,0.050473186,1.0,1.0,14.0,0.0,12032.0,162.0,10.0,0.098765432,0.082568807])]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#将类别特征和数值特征合并\n",
    "def build_feature(x):\n",
    "    import numpy as np\n",
    "    numeric_feature = [0.0 if yy=='?' else yy for yy in [y.replace('\\\"','') for y in x[4:-1]]]  #数值特征\n",
    "    category_feature = np.zeros(num_categories)              #数值特征\n",
    "    category_index = category_dict[x[3].replace('\\\"','')]    \n",
    "    category_feature[category_index] = 1                     #类别特征\n",
    "    label = x[-1].replace('\\\"','')                           #样本标签\n",
    "    feature = LabeledPoint(label, Vectors.dense(list(category_feature)+numeric_feature))   #合并类别特征和数值特征\n",
    "    return feature\n",
    "\n",
    "category_data = trimmed.map(lambda x:build_feature(x))\n",
    "category_data.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "规范化之前的数据集特征: [DenseVector([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.7891, 2.0556, 0.6765, 0.2059, 0.0471, 0.0235, 0.4438, 0.0, 0.0, 0.0908, 0.0, 0.2458, 0.0039, 1.0, 1.0, 24.0, 0.0, 5424.0, 170.0, 8.0, 0.1529, 0.0791]), DenseVector([1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5741, 3.678, 0.508, 0.2888, 0.2139, 0.1444, 0.4686, 0.0, 0.0, 0.0987, 0.0, 0.2035, 0.0887, 1.0, 1.0, 40.0, 0.0, 4973.0, 187.0, 9.0, 0.1818, 0.1254]), DenseVector([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9965, 2.3829, 0.562, 0.3217, 0.1202, 0.0426, 0.5254, 0.0, 0.0, 0.0724, 0.0, 0.2264, 0.1205, 1.0, 1.0, 55.0, 0.0, 2240.0, 258.0, 11.0, 0.1667, 0.0576]), DenseVector([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8012, 1.5431, 0.4, 0.1, 0.0167, 0.0, 0.4807, 0.0, 0.0, 0.0959, 0.0, 0.2657, 0.0353, 1.0, 0.0, 24.0, 0.0, 2737.0, 120.0, 5.0, 0.0417, 0.1009]), DenseVector([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.7192, 2.6765, 0.5, 0.2222, 0.1235, 0.0432, 0.4461, 0.0, 0.0, 0.0249, 0.0, 0.2289, 0.0505, 1.0, 1.0, 14.0, 0.0, 12032.0, 162.0, 10.0, 0.0988, 0.0826])]\n",
      "规范化之后的数据集特征: [DenseVector([-0.4464, -0.2205, -0.0649, -0.0233, -0.0991, -0.2042, -0.2017, -0.0285, -0.271, -0.2327, -0.6808, -0.3818, -0.1019, 2.7207, 1.1376, -0.0819, 1.0251, -0.0559, -0.4689, -0.3543, -0.3175, 0.3385, 0.0, 0.8288, -0.1473, 0.2296, -0.1416, 0.7902, 0.7172, -0.298, -0.2035, -0.033, -0.0488, 0.9401, -0.1087, -0.2788]), DenseVector([2.2397, -0.2205, -0.0649, -0.0233, -0.0991, -0.2042, -0.2017, -0.0285, -0.271, -0.2327, -0.6808, -0.3818, -0.1019, -0.3675, 0.4887, 0.1063, 0.1959, 0.509, 1.2695, 1.3097, -0.3132, 0.3385, 0.0, 1.0202, -0.1473, -0.5771, -0.0975, 0.7902, 0.7172, 0.4866, -0.2035, -0.0838, 0.0459, 1.2494, 0.0489, 0.3058]), DenseVector([-0.4464, -0.2205, -0.0649, -0.0233, -0.0991, -0.2042, -0.2017, -0.0285, 3.6896, -0.2327, -0.6808, -0.3818, -0.1019, -0.3675, 1.7637, -0.044, 0.4617, 0.7334, 0.2927, -0.0912, -0.3032, 0.3385, 0.0, 0.3867, -0.1473, -0.1405, -0.0808, 0.7902, 0.7172, 1.2221, -0.2035, -0.3917, 0.4416, 1.868, -0.0338, -0.5504]), DenseVector([-0.4464, -0.2205, -0.0649, -0.0233, -0.0991, -0.2042, -0.2017, -0.0285, 3.6896, -0.2327, -0.6808, -0.3818, -0.1019, -0.3675, 1.1742, -0.1414, -0.3359, -0.7774, -0.7856, -0.6783, -0.3111, 0.3385, 0.0, 0.9516, -0.1473, 0.6073, -0.1252, 0.7902, -1.3941, -0.298, -0.2035, -0.3357, -0.3274, 0.0122, -0.7158, -0.0046]), DenseVector([-0.4464, -0.2205, -0.0649, -0.0233, -0.0991, -0.2042, -0.2017, -0.0285, -0.271, 4.2963, -0.6808, -0.3818, -0.1019, -0.3675, 0.9264, -0.0099, 0.1564, 0.0555, 0.3271, -0.0833, -0.3171, 0.3385, 0.0, -0.7604, -0.1473, -0.0932, -0.1174, 0.7902, 0.7172, -0.7884, -0.2035, 0.7116, -0.0934, 1.5587, -0.4043, -0.2354])]\n"
     ]
    }
   ],
   "source": [
    "#同样,因为我们的原始数据没有标准化,所以在训练这个扩展数据集之前,应该使用同样的StandardScaler方法对其进行标准化转换: \n",
    "category_labels = category_data.map(lambda point: point.label)\n",
    "category_features = category_data.map(lambda point: point.features)\n",
    "scaler2 = StandardScaler(withMean=True, withStd=True).fit(category_features)\n",
    "print ('规范化之前的数据集特征:',category_features.take(5))\n",
    "scaled_category_data = category_labels.zip(scaler2.transform(category_features))\n",
    "scaled_category_data = scaled_category_data.map(lambda kv: LabeledPoint(kv[0],kv[1]))\n",
    "print ('规范化之后的数据集特征:',scaled_category_data.map(lambda point: point.features).take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda1\\lib\\site-packages\\pyspark\\mllib\\classification.py:313: UserWarning: Deprecated in 2.0.0. Use ml.classification.LogisticRegression or LogisticRegressionWithLBFGS.\n",
      "  \"Deprecated in 2.0.0. Use ml.classification.LogisticRegression or \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "逻辑回归模型的准确率 : 0.665720\n"
     ]
    }
   ],
   "source": [
    "#计算模型准确率\n",
    "lrModel_category_scaled = LogisticRegressionWithSGD.train(scaled_category_data, numIteration)\n",
    "lr_totalCorrect_category_scaled = scaled_category_data.map(lambda point : 1 if(lrModel_category_scaled.predict(point.features)==point.label) else 0).sum()\n",
    "lr_accuracy_category_scaled = lr_totalCorrect_category_scaled/(1.0*data.count())\n",
    "print ('逻辑回归模型的准确率 : %f'%lr_accuracy_category_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegressionModel的AUC是0.665475,PR是0.757982\n"
     ]
    }
   ],
   "source": [
    "#计算模型的AUC和PR\n",
    "all_models_metrics =[]\n",
    "for model in [lrModel_category_scaled]:\n",
    "    scoresAndLabels = scaled_category_data.map(lambda point:(model.predict(point.features),point.label)).collect()\n",
    "    scoresAndLabels = [(float(i),j) for (i,j) in scoresAndLabels]\n",
    "    scoresAndLabels_rdd = sc.parallelize(scoresAndLabels)\n",
    "    metrics = BinaryClassificationMetrics(scoresAndLabels_rdd)\n",
    "    all_models_metrics.append((model.__class__.__name__,metrics.areaUnderROC, metrics.areaUnderPR))\n",
    "\n",
    "for model_name, AUC, PR in all_models_metrics:\n",
    "    print ('%s的AUC是%f,PR是%f'%(model_name, AUC, PR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------\n",
    "6.3 使用正确的数据格式 \n",
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "模型性能的另外一个关键部分是对每个模型使用正确的数据格式。前面对数值向量应用朴素贝叶斯模型得到了非常差的结果，这是模型本身的缺陷还是因为其他原因造成的呢？在这里,我们知道MLlib实现了多项式模型,并且该模型可以处理计数形式的数据。这包括二元表示的类型特征(比如前面提到的1-of-k表示)或者频率数据(比如一个文档中单词出现的频率)。我开始时使用的数值特征并不符合假定的输入分布,所以模型性能不好也并不是意料之外。 \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "为了更好的说明问题，本文仅仅使用类型特征,而1-of-k编码的类型特征更符合朴素贝叶斯模型,我们用如下代码构建数据集: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "category_nbdata = scaled_category_data\n",
    "category_nbdata_nonegative = category_nbdata.map(lambda point: (point.label,point.features)).map(lambda kv: (kv[0],[0.0 if yy<0.0 else yy for yy in kv[1]])).map(lambda kv:LabeledPoint(kv[0],Vectors.dense(kv[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(0.0, [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,2.72073665645,1.1376473365,0.0,1.02513981289,0.0,0.0,0.0,0.0,0.33845079824,0.0,0.828822173315,0.0,0.229639823578,0.0,0.790238049918,0.717194729453,0.0,0.0,0.0,0.0,0.940069975117,0.0,0.0]),\n",
       " LabeledPoint(1.0, [2.23973405107,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.488685990417,0.106283637051,0.195885662909,0.508986806825,1.26946916328,1.30971389846,0.0,0.33845079824,0.0,1.02024383053,0.0,0.0,0.0,0.790238049918,0.717194729453,0.486582251769,0.0,0.0,0.0459442290216,1.24936955983,0.0488534204631,0.305780221901])]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_nbdata_nonegative.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "朴素贝叶斯模型的准确率:0.652738\n"
     ]
    }
   ],
   "source": [
    "#计算准确率\n",
    "nb_category_model = NaiveBayes.train(category_nbdata_nonegative)\n",
    "nb_category_total_correct = category_nbdata_nonegative.map(lambda point:1 if (nb_category_model.predict(point.features)==point.label) else 0).sum()\n",
    "\n",
    "nb_category_accuracy = nb_category_total_correct/(1.0*category_nbdata_nonegative.count())\n",
    "print ('朴素贝叶斯模型的准确率:%f'%nb_category_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaiveBayesModel的AUC是0.651468,PR是0.752038\n"
     ]
    }
   ],
   "source": [
    "all_models_metrics =[]\n",
    "for model in [nb_category_model]:\n",
    "    scoresAndLabels = category_nbdata_nonegative.map(lambda point:(model.predict(point.features),point.label)).collect()\n",
    "    scoresAndLabels = [(float(i),j) for (i,j) in scoresAndLabels]\n",
    "    scoresAndLabels_rdd = sc.parallelize(scoresAndLabels)\n",
    "    metrics = BinaryClassificationMetrics(scoresAndLabels_rdd)\n",
    "    all_models_metrics.append((model.__class__.__name__,metrics.areaUnderROC, metrics.areaUnderPR))\n",
    "\n",
    "for model_name, AUC, PR in all_models_metrics:\n",
    "    print ('%s的AUC是%f,PR是%f'%(model_name, AUC, PR))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上述结果可知,使用格式正确的输入数据后 ,朴素贝叶斯的准确率从58%提高到了65.27%。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------\n",
    "6.4 模型参数调优 \n",
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------\n",
    "6.4.1 逻辑回归\n",
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前几节展示了模型性能的影响因素:特征提取、特征选择、输入数据的格式和模型对数据分布的假设。但是到目前为止,我们对模型参数的讨论只是一笔带过,而实际上它对于模型性能影响很大。\n",
    "\n",
    "MLlib默认的train方法对每个模型的参数都使用默认值。接下来让我们深入了解一下这些参数。 下面就演示一下在Spark Python中我们如何进行这部分的工作。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_with_params(input_data, model, reg_param, num_iter, step_size):\n",
    "    model = model.train(input_data, iterations=num_iter, regParam=reg_param, step=step_size)\n",
    "    return model\n",
    "def create_metrics(tag, input_data, model):\n",
    "    scoresAndLabels_rdd = input_data.map(lambda x: (model.predict(x.features)*1.0,x.label*1.0))\n",
    "    metrics = BinaryClassificationMetrics(scoresAndLabels_rdd)\n",
    "    return tag, metrics.areaUnderROC, metrics.areaUnderPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda1\\lib\\site-packages\\pyspark\\mllib\\classification.py:313: UserWarning: Deprecated in 2.0.0. Use ml.classification.LogisticRegression or LogisticRegressionWithLBFGS.\n",
      "  \"Deprecated in 2.0.0. Use ml.classification.LogisticRegression or \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 iterations,ROC = 0.649520,PR=0.745886\n",
      "5 iterations,ROC = 0.666161,PR=0.758022\n",
      "10 iterations,ROC = 0.665483,PR=0.757964\n",
      "50 iterations,ROC = 0.668143,PR=0.760930\n"
     ]
    }
   ],
   "source": [
    "#对参数迭代次数进行调优\n",
    "for iteration_num in [1,5,10,50]:\n",
    "    model = train_with_params(scaled_category_data, LogisticRegressionWithSGD, 0.0, iteration_num, 1.0)\n",
    "    label, ROC, PR = create_metrics('%d iterations'%iteration_num,scaled_category_data,model)\n",
    "    print ('%s,ROC = %f,PR=%f'%(label,ROC,PR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上述结果可知，迭代次数达到一定值之后，再增加迭代次数对结果影响已经较小了。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对参数步长进行调优：\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "在SGD中,在训练每个样本并更新模型的权重向量时,步长用来控制算法在最陡的梯度方向上应该前进多远。较大的步长收敛较快,但是步长太大可能导致收敛到局部最优解。下面计算不同步长的影响：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda1\\lib\\site-packages\\pyspark\\mllib\\classification.py:313: UserWarning: Deprecated in 2.0.0. Use ml.classification.LogisticRegression or LogisticRegressionWithLBFGS.\n",
      "  \"Deprecated in 2.0.0. Use ml.classification.LogisticRegression or \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001 的步长,ROC = 0.649659,PR=0.745974\n",
      "0.010 的步长,ROC = 0.649644,PR=0.746017\n",
      "0.100 的步长,ROC = 0.655211,PR=0.750008\n",
      "1.000 的步长,ROC = 0.665483,PR=0.757964\n",
      "10.000 的步长,ROC = 0.619228,PR=0.725713\n"
     ]
    }
   ],
   "source": [
    "#对参数步长次数进行调优\n",
    "for step_size in [0.001, 0.01, 0.1, 1.0, 10.0]:\n",
    "    model = train_with_params(scaled_category_data, LogisticRegressionWithSGD, 0.0, 10, step_size)\n",
    "    label, ROC, PR = create_metrics('%2.3f 的步长'%step_size,scaled_category_data,model)\n",
    "    print ('%s,ROC = %f,PR=%f'%(label,ROC,PR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上述结果可以看出，步长增长过大对模型性能有负面影响。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对参数正则化系数进行调优： \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "正则化的具体做法是在损失函数中添加一项关于模型权重向量的函数,从而会使损失增加。正则化在现实中几乎是必须的,当特征维度高于训练样本时(此时变量相关需要学习的权重数量也非常大)尤其重要。\n",
    "\n",
    "当正则化不存在或者非常低时,模型容易过拟合。而且大多数模型在没有正则化的情况会在训练数据上过拟合。过拟合也是交叉验证技术使用的关键原因,交叉验证会在后面详细介绍。\n",
    "\n",
    "相反,虽然正则化可以得到一个简单模型,但正则化太高可能导致模型欠拟合,从而使模型性能变得很糟糕。 \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "MLlib中可用的正则化形式有如下几个。\n",
    "\n",
    " SimpleUpdater:相当于没有正则化,是逻辑回归的默认配置。SquaredL2Updater:这个正则项基于权重向量的L2正则化,是SVM模型的默认值。\n",
    "\n",
    " L1Updater:这个正则项基于权重向量的L1正则化,会导致得到一个稀疏的权重向量(不重要的权重的值接近0)。 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda1\\lib\\site-packages\\pyspark\\mllib\\classification.py:313: UserWarning: Deprecated in 2.0.0. Use ml.classification.LogisticRegression or LogisticRegressionWithLBFGS.\n",
      "  \"Deprecated in 2.0.0. Use ml.classification.LogisticRegression or \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正则化系数0.001 ,ROC = 0.649644,PR=0.746017\n",
      "正则化系数0.010 ,ROC = 0.649644,PR=0.746017\n",
      "正则化系数0.100 ,ROC = 0.649644,PR=0.746017\n",
      "正则化系数1.000 ,ROC = 0.649644,PR=0.746017\n",
      "正则化系数10.000 ,ROC = 0.648979,PR=0.745492\n"
     ]
    }
   ],
   "source": [
    "#对参数正则化系数进行调优\n",
    "for reg_param in [0.001, 0.01, 0.1, 1.0, 10.0]:\n",
    "    model = train_with_params(scaled_category_data, LogisticRegressionWithSGD, reg_param, 10, 0.01)\n",
    "    label, ROC, PR = create_metrics('正则化系数%2.3f '%reg_param,scaled_category_data,model)\n",
    "    print ('%s,ROC = %f,PR=%f'%(label,ROC,PR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------\n",
    "6.4.2 决策树\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "决策树模型在一开始使用原始数据做训练时获得了最好的性能。当时设置了参数maxDepth用来控制决策树的最大深度,进而控制模型的复杂度。而树的深度越大,得到的模型越复杂,但有能力更好地拟合数据。对于分类问题,我们需要为决策树模型选择以下两种不纯度度量方式:Gini或者Entropy。 \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "对树的深度和不纯度调优 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_with_params_dt(input_data, impurity, maxTreeDepth):\n",
    "    dt_model = DecisionTree.trainClassifier(input_data,numClass,{},impurity, maxDepth=maxTreeDepth)\n",
    "    return dt_model\n",
    "def create_metrics_dt(tag, input_data, model):\n",
    "    predictLabel= model.predict(input_data.map(lambda point: point.features)).collect()\n",
    "    trueLabel = input_data.map(lambda point: point.label).collect()\n",
    "    scoresAndLabels = [(predictLabel[i],true_val) for i, true_val in enumerate(trueLabel)]\n",
    "    scoresAndLabels_rdd = sc.parallelize(scoresAndLabels)\n",
    "    scoresAndLabels_rdd = scoresAndLabels_rdd.map(lambda kv: (float(kv[0]),float(kv[1])))\n",
    "    dt_metrics = BinaryClassificationMetrics(scoresAndLabels_rdd)\n",
    "    return tag,dt_metrics.areaUnderROC,dt_metrics.areaUnderPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "impurity: entropy, 1 maxTreeDepth:, AUC=0.593268, PR=0.749004\n",
      "impurity: gini, 1 maxTreeDepth:, AUC=0.593268, PR=0.749004\n",
      "impurity: entropy, 2 maxTreeDepth:, AUC=0.616839, PR=0.725164\n",
      "impurity: gini, 2 maxTreeDepth:, AUC=0.616839, PR=0.725164\n",
      "impurity: entropy, 3 maxTreeDepth:, AUC=0.626070, PR=0.751813\n",
      "impurity: gini, 3 maxTreeDepth:, AUC=0.626070, PR=0.751813\n",
      "impurity: entropy, 4 maxTreeDepth:, AUC=0.636333, PR=0.749393\n",
      "impurity: gini, 4 maxTreeDepth:, AUC=0.636333, PR=0.749393\n",
      "impurity: entropy, 5 maxTreeDepth:, AUC=0.648837, PR=0.743081\n",
      "impurity: gini, 5 maxTreeDepth:, AUC=0.648916, PR=0.742894\n",
      "impurity: entropy, 10 maxTreeDepth:, AUC=0.762552, PR=0.829623\n",
      "impurity: gini, 10 maxTreeDepth:, AUC=0.783709, PR=0.843469\n",
      "impurity: entropy, 20 maxTreeDepth:, AUC=0.984537, PR=0.988522\n",
      "impurity: gini, 20 maxTreeDepth:, AUC=0.988707, PR=0.991328\n"
     ]
    }
   ],
   "source": [
    "for depth in [1,2,3,4,5,10,20]:\n",
    "    for im in ['entropy','gini']:\n",
    "        model=train_with_params_dt(data,im,depth)\n",
    "        tag, ROC, PR = create_metrics_dt('impurity: %s, %d maxTreeDepth:'%(im,depth),data,model)\n",
    "        print ('%s, AUC=%f, PR=%f'%(tag,ROC,PR))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从结果中可以看出,提高树的深度可以得到更精确的模型(这和预期一致,因为模型在更大的树深度下会变得更加复杂)。然而树的深度越大,模型对训练数据过拟合程度越严重。另外，两种不纯度方法对性能的影响差异不大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------\n",
    "6.4.3 朴素贝叶斯\n",
    "----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后,让我们看看lamda参数对朴素贝叶斯模型的影响。该参数可以控制相加式平滑(additive smoothing),解决数据中某个类别和某个特征值的组合没有同时出现的问题 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_with_params_nb(input_data, lambda_para):\n",
    "    nb_model = NaiveBayes.train(input_data,lambda_para)\n",
    "    return nb_model\n",
    "def create_metrics_nb(tag, nbdata, model):\n",
    "    scoresAndLabels = nbdata.map(lambda point:(float(model.predict(point.features)),point.label))\n",
    "    nb_metrics = BinaryClassificationMetrics(scoresAndLabels)\n",
    "    return tag,nb_metrics.areaUnderROC,nb_metrics.areaUnderPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda=0.001000, AUC=0.583697, PR=0.680961\n",
      "lambda=0.010000, AUC=0.583697, PR=0.680961\n",
      "lambda=0.100000, AUC=0.583697, PR=0.680961\n",
      "lambda=1.000000, AUC=0.583559, PR=0.680851\n",
      "lambda=10.000000, AUC=0.583412, PR=0.680762\n"
     ]
    }
   ],
   "source": [
    "for lambda_para in [0.001, 0.01, 0.1, 1.0, 10.0]:\n",
    "    model=train_with_params_nb(nbdata,lambda_para)\n",
    "    tag, ROC, PR = create_metrics_dt('lambda=%f' %lambda_para,nbdata,model)\n",
    "    print ('%s, AUC=%f, PR=%f'%(tag,ROC,PR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从结果中可以看出lambda的值对性能没有影响,由此可见数据中某个特征和某个类别的组合不存在时不是问题。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------\n",
    "6.4.4 交叉验证\n",
    "-------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "交叉验证是实际机器学习中的关键部分，同时在多模型选择和参数调优中占有很重的地位。\n",
    "\n",
    "\n",
    "\n",
    "交叉验证的目的是测试模型在未知数据上的性能。不知道训练的模型在预测新数据时的性能,而直接放在实际数据(比如运行的系统)中进行评估是很危险的做法。正如前面提到的正则化实验中,我们的模型可能在训练数据中已经过拟合了,于是在未被训练的新数据中预测性能会很差。\n",
    "\n",
    "\n",
    "\n",
    "交叉验证让我们使用一部分数据训练模型,将另外一部分用来评估模型性能。如果模型在训练以外的新数据中进行了测试,我们便可以由此估计模型对新数据的泛化能力。我们把数据划分为训练和测试数据,实现一个简单的交叉验证过程。我们将数据分为两个不重叠的数据集。第一个数据集用来训练,称为训练集。第二个数据集称为测试集或者保留集,用来评估模型在给定评测方法下的性能。实际中常用的划分方法包括:50/50、60/40、80/20等,只要训练模型的数据量不太小就行(通常,实际使用至少50%的数据用于训练)。 \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "在建模过程中,通常会创建三个数据集:训练集、评估集(类似上述测试集用于模型参数的调优,比如lambda和步长)和测试集(不用于模型的训练和参数调优,只用于估计模型在新数据中性能)。 \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "本文将数据集分成60%的训练集和40%的测试集(为了方便解释,我们在代码中使用一个固定的随机种子123来保证每次实验能得到相同的结果): \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda1\\lib\\site-packages\\pyspark\\mllib\\classification.py:313: UserWarning: Deprecated in 2.0.0. Use ml.classification.LogisticRegression or LogisticRegressionWithLBFGS.\n",
      "  \"Deprecated in 2.0.0. Use ml.classification.LogisticRegression or \"\n"
     ]
    },
    {
     "ename": "IllegalArgumentException",
     "evalue": "'requirement failed: Initial step size must be positive but got 0.0'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mD:\\anaconda1\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda1\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    318\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    320\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o2333.trainLogisticRegressionModelWithSGD.\n: java.lang.IllegalArgumentException: requirement failed: Initial step size must be positive but got 0.0\r\n\tat scala.Predef$.require(Predef.scala:224)\r\n\tat org.apache.spark.mllib.optimization.GradientDescent.setStepSize(GradientDescent.scala:49)\r\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.trainLogisticRegressionModelWithSGD(PythonMLLibAPI.scala:274)\r\n\tat sun.reflect.GeneratedMethodAccessor171.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-93802becf388>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mreg_para\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0025\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.005\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_with_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLogisticRegressionWithSGD\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreg_para\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mROC\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPR\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%f regularization parameter'\u001b[0m\u001b[1;33m%\u001b[0m\u001b[0mreg_para\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'%s,AUC = %f,PR=%f'\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mROC\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mPR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-31-8039be79b29c>\u001b[0m in \u001b[0;36mtrain_with_params\u001b[1;34m(input_data, model, reg_param, num_iter, step_size)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtrain_with_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreg_param\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregParam\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreg_param\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcreate_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mscoresAndLabels_rdd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda1\\lib\\site-packages\\pyspark\\mllib\\classification.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(cls, data, iterations, step, miniBatchFraction, initialWeights, regParam, regType, intercept, validateData, convergenceTol)\u001b[0m\n\u001b[0;32m    319\u001b[0m                                  bool(intercept), bool(validateData), float(convergenceTol))\n\u001b[0;32m    320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_regression_train_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLogisticRegressionModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitialWeights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda1\\lib\\site-packages\\pyspark\\mllib\\regression.py\u001b[0m in \u001b[0;36m_regression_train_wrapper\u001b[1;34m(train_func, modelClass, data, initial_weights)\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmodelClass\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mLogisticRegressionModel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m         weights, intercept, numFeatures, numClasses = train_func(\n\u001b[1;32m--> 215\u001b[1;33m             data, _convert_to_vector(initial_weights))\n\u001b[0m\u001b[0;32m    216\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmodelClass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mintercept\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumFeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumClasses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda1\\lib\\site-packages\\pyspark\\mllib\\classification.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(rdd, i)\u001b[0m\n\u001b[0;32m    317\u001b[0m             return callMLlibFunc(\"trainLogisticRegressionModelWithSGD\", rdd, int(iterations),\n\u001b[0;32m    318\u001b[0m                                  \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mminiBatchFraction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mregParam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregType\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m                                  bool(intercept), bool(validateData), float(convergenceTol))\n\u001b[0m\u001b[0;32m    320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_regression_train_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLogisticRegressionModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitialWeights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda1\\lib\\site-packages\\pyspark\\mllib\\common.py\u001b[0m in \u001b[0;36mcallMLlibFunc\u001b[1;34m(name, *args)\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[0mapi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonMLLibAPI\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcallJavaFunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mapi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda1\\lib\\site-packages\\pyspark\\mllib\\common.py\u001b[0m in \u001b[0;36mcallJavaFunc\u001b[1;34m(sc, func, *args)\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[1;34m\"\"\" Call Java Function \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_py2java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_java2py\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda1\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1133\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda1\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     77\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m             \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIllegalArgumentException\u001b[0m: 'requirement failed: Initial step size must be positive but got 0.0'"
     ]
    }
   ],
   "source": [
    "train_test_split = scaled_category_data.randomSplit([0.6,0.4],123)\n",
    "train = train_test_split[0]\n",
    "test = train_test_split[1]\n",
    "for reg_para in [0.0, 0.001, 0.0025, 0.005, 0.01]:\n",
    "    model = train_with_params(train, LogisticRegressionWithSGD, 0.0, 1.0, reg_para)\n",
    "    label, ROC, PR = create_metrics('%f regularization parameter'%reg_para,test,model)\n",
    "    print ('%s,AUC = %f,PR=%f'%(label,ROC,PR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
