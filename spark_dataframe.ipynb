{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark import rdd\n",
    "conf=SparkConf().setMaster(\"local[*]\").setAppName(\"First_APP\")\n",
    "sc=SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext=SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['40920', '8.326976', '0.953952', 'largeDoses'],\n",
       " ['14488', '7.153469', '1.673904', 'smallDoses']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_content = sc.textFile('E:/machine_data/dataframe/ab.csv')\n",
    "df = file_content.map(lambda x:x.split(','))\n",
    "df.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+--------+----------+\n",
      "|Mileage |Gametime |Icecream|label     |\n",
      "+--------+---------+--------+----------+\n",
      "|40920   |8.326976 |0.953952|largeDoses|\n",
      "|14488   |7.153469 |1.673904|smallDoses|\n",
      "|26052   |1.441871 |0.805124|didntLike |\n",
      "|75136   |13.147394|0.428964|didntLike |\n",
      "|38344   |1.669788 |0.134296|didntLike |\n",
      "+--------+---------+--------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[Mileage : string, Gametime: string, Icecream: string, label: string]>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#将数据集转换成dataframe格式\n",
    "dataset = sqlContext.createDataFrame(df, ['Mileage ', 'Gametime', 'Icecream', 'label'])\n",
    "dataset.show(5, False)\n",
    "dataset.printSchema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'didntLike': 2, 'largeDoses': 0, 'smallDoses': 1}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#建立标签label的索引字典，将字符串型的label转换成数值型的label。\n",
    "label_set = dataset.rdd.map(lambda x: x[3]).distinct().collect()\n",
    "label_dict = dict()\n",
    "i = 0\n",
    "for key in label_set:\n",
    "    if key not in label_dict.keys():\n",
    "        label_dict[key ]= i\n",
    "        i = i+1\n",
    "label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+--------+-----+\n",
      "|Mileage |Gametime |Icecream|label|\n",
      "+--------+---------+--------+-----+\n",
      "|40920   |8.326976 |0.953952|0    |\n",
      "|14488   |7.153469 |1.673904|1    |\n",
      "|26052   |1.441871 |0.805124|2    |\n",
      "|75136   |13.147394|0.428964|2    |\n",
      "|38344   |1.669788 |0.134296|2    |\n",
      "+--------+---------+--------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[Mileage : bigint, Gametime: double, Icecream: double, label: bigint]>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = dataset.rdd.map(lambda x: ([x[i] for i in range(3)], label_dict[x[3]])).\\\n",
    "               map(lambda kv: [int(kv[0][0]), float(kv[0][1]), float(kv[0][2]), kv[1]])\n",
    "data = sqlContext.createDataFrame(data,  ['Mileage ', 'Gametime', 'Icecream', 'label'] )\n",
    "data.show(5, False)\n",
    "data.printSchema\n",
    "#data.selectExpr('Mileage', 'Gametime', 'Icecream', 'label').show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在数据集已经符合我们的要求了，接下来就是建立模型了。在建立模型之前，我先对其进行标准化，然后用主成份分析（PCA）进行了降维，最后通过逻辑回归（logistic）模型进行分类和概率预测。具体实现代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+----------+----------+\n",
      "|probability                 |prediction|prediction|\n",
      "+----------------------------+----------+----------+\n",
      "|[1.0,1.7947449461974263E-20]|0.0       |0.0       |\n",
      "|[1.0,3.800215846574964E-25] |0.0       |0.0       |\n",
      "+----------------------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "# $example on$\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "\n",
    "# 将类别2和类别1合并，即Helen对男生的印象是要么有魅力要么没有魅力。\n",
    "# 之所以合并，是因为pyspark.ml.classification.LogisticRegression目前仅支持二分类\n",
    "feature_data = data.rdd.map(lambda x:(Vectors.dense([x[i] for i in range(0,3)]),float(1 if x[3]==2 else x[3])))\n",
    "feature_data = sqlContext.createDataFrame(feature_data, ['features', 'labels'])\n",
    "#feature_data.show()\n",
    "\n",
    "train_data, test_data = feature_data.randomSplit([0.7, 0.3], 6)\n",
    "#train.show()\n",
    "\n",
    "scaler = StandardScaler(inputCol='features', outputCol='scaledFeatures',\n",
    "                            withStd=True, withMean=False)\n",
    "pca = PCA(k=2, inputCol=\"scaledFeatures\", outputCol=\"pcaFeatures\")\n",
    "lr = LogisticRegression(maxIter=10, featuresCol='pcaFeatures', labelCol='labels')\n",
    "\n",
    "\n",
    "pipeline = Pipeline(stages=[scaler, pca, lr])\n",
    "\n",
    "Model = pipeline.fit(train_data)\n",
    "results = Model.transform(test_data)\n",
    "\n",
    "results.select('probability', 'prediction', 'prediction').show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|            features|labels|      scaledFeatures|         pcaFeatures|       rawPrediction|         probability|prediction|\n",
      "+--------------------+------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|[26052.0,1.441871...|   1.0|[0.85677243949982...|[-0.0203392525195...|[45.4668389392915...|[1.0,1.7947449461...|       0.0|\n",
      "|[38344.0,1.669788...|   1.0|[1.26101959236071...|[-0.9155042464058...|[56.2295694580017...|[1.0,3.8002158465...|       0.0|\n",
      "+--------------------+------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print (results.show(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+------+----------+\n",
      "|probability                 |labels|prediction|\n",
      "+----------------------------+------+----------+\n",
      "|[1.0,1.7947449461974263E-20]|1.0   |0.0       |\n",
      "|[1.0,3.800215846574964E-25] |1.0   |0.0       |\n",
      "+----------------------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.select('probability', 'labels', 'prediction').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#最后对模型进行简单的评估\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "predictionAndLabels = results.select('probability', 'labels', 'prediction').rdd.map(lambda x: (x[1], x[2]))\n",
    "metrics = MulticlassMetrics(predictionAndLabels)\n",
    "metrics.confusionMatrix().toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
