{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark import rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conf=SparkConf().setMaster(\"local[*]\").setAppName(\"First_APP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc=SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    }
   ],
   "source": [
    "data=sc.parallelize(range(10))\n",
    "ans=data.reduce(lambda x,y:x+y)\n",
    "print (ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104\n",
      "# Apache Spark\n"
     ]
    }
   ],
   "source": [
    "lines=sc.textFile(\"E:\\Spark\\spark-2.1.0-bin-hadoop2.7\\README.md\")\n",
    "print (lines.count())\n",
    "print (lines.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MyClass(object):\n",
    "    def __init__(self):\n",
    "        self.field = 1\n",
    "\n",
    "    def doStuff(self, rdd):\n",
    "        #将需要的字段提取到局部变量中即可\n",
    "        field = self.field\n",
    "        #return rdd.map(lambda s: self.field + x)是错误的\n",
    "        #如果你传递的对象是某个对象的成员，或者在某个函数中引用了一个整个字段，会报错\n",
    "        return rdd.map(lambda s: field + s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.MyClass object at 0x000001DD4DB4CE48>\n"
     ]
    }
   ],
   "source": [
    "my=MyClass()\n",
    "data=sc.parallelize(range(10))\n",
    "my.doStuff(data)\n",
    "print (my)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pairs=lines.map(lambda x:(x.split(\" \")[0],x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'Spark is a fast and general cluster computing system for Big Data. It provides', 'high-level APIs in Scala, Java, Python, and R, and an optimized engine that', 'supports general computation graphs for data analysis. It also supports a', 'rich set of higher-level tools including Spark SQL for SQL and DataFrames,', 'MLlib for machine learning, GraphX for graph processing,', 'and Spark Streaming for stream processing.', '', '<http://spark.apache.org/>']\n"
     ]
    }
   ],
   "source": [
    "print (lines.collect()[1:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('', ''), ('Spark', 'Spark is a fast and general cluster computing system for Big Data. It provides'), ('high-level', 'high-level APIs in Scala, Java, Python, and R, and an optimized engine that'), ('supports', 'supports general computation graphs for data analysis. It also supports a'), ('rich', 'rich set of higher-level tools including Spark SQL for SQL and DataFrames,'), ('MLlib', 'MLlib for machine learning, GraphX for graph processing,'), ('and', 'and Spark Streaming for stream processing.'), ('', ''), ('<http://spark.apache.org/>', '<http://spark.apache.org/>')]\n"
     ]
    }
   ],
   "source": [
    "print (pairs.collect()[1:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('', ''), ('', ''), ('', ''), ('', ''), ('', ''), ('', ''), ('##', '## Building Spark'), ('', ''), ('', '')]\n"
     ]
    }
   ],
   "source": [
    "#用Python对第二个元素进行筛选\n",
    "result = pairs.filter(lambda keyValue:len(keyValue[1]) < 20)\n",
    "print (result.collect()[1:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#在Python中使用reduceByKey()和mapValues()计算每个键对应的平均值\n",
    "data=sc.parallelize([1,2,3,4,5,3,2])\n",
    "data=data.map(lambda x:(x,1))\n",
    "data1=data.mapValues(lambda x:(x,1)) #mapValues只接收KV键对的RDD数据，map则不用\n",
    "data2=data1.reduceByKey(lambda x,y:(x[0]+y[0],x[1]+y[1])) #key相同的进行聚合\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (3, 1), (2, 1)]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, (1, 1)),\n",
       " (2, (1, 1)),\n",
       " (3, (1, 1)),\n",
       " (4, (1, 1)),\n",
       " (5, (1, 1)),\n",
       " (3, (1, 1)),\n",
       " (2, (1, 1))]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.collect()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, (1, 1)), (1, (1, 1)), (5, (1, 1)), (2, (2, 2)), (3, (2, 2))]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.collect()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#用Python实现单词计数\n",
    "rdd=sc.textFile(\"E:\\Spark\\spark-2.1.0-bin-hadoop2.7\\README.md\")\n",
    "words = rdd.flatMap(lambda x:x.split(\" \"))\n",
    "result = words.map(lambda x:(x,1)).reduceByKey(lambda x,y:x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#', 'Apache', 'Spark', '', 'Spark', 'is', 'a', 'fast', 'and', 'general']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.collect()[0:10] #''代表哪一行是空的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 72),\n",
       " ('guide,', 1),\n",
       " ('APIs', 1),\n",
       " ('optimized', 1),\n",
       " ('name', 1),\n",
       " ('contributing', 1),\n",
       " ('developing', 1),\n",
       " ('It', 2),\n",
       " ('package.', 1),\n",
       " ('particular', 2)]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.collect()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#在Python中使用combineByKey()求每个键对应的平均值\n",
    "nums=sc.parallelize([1,2,3,4,5,3,2])\n",
    "nums=nums.map(lambda x:(x,1))\n",
    "sumCount = nums.combineByKey((lambda x:(x,1)),\n",
    "                             (lambda x,y:(x[0]+y,x[1]+1)),\n",
    "                             (lambda x,y:(x[0]+y[0],x[1]+y[1])))\n",
    "#sumCount.map(lambda key,xy:(key.xy[0]/xy[1])).collectAsMap()\n",
    "#sumCount.map(lambda key,xy:(key.xy[0]/xy[1])).collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, (1, 1)), (1, (1, 1)), (5, (1, 1)), (2, (2, 2)), (3, (2, 2))]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sumCount.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data4=data1.reduceByKey(lambda x,y:(x[0]+1,x[1]+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, (1, 1)), (1, (1, 1)), (5, (1, 1)), (2, (2, 2)), (3, (2, 2))]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data4.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nums1=sc.parallelize([1,2,3,4,5,3,2])\n",
    "nums2=nums1.map(lambda x:(x,1))\n",
    "nums3=nums2.reduceByKey(lambda x,y:x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (3, 1), (2, 1)]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 1), (1, 1), (5, 1), (2, 2), (3, 2)]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums5=nums3.union(nums3)\n",
    "nums3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 1),\n",
       " (4, 1),\n",
       " (1, 1),\n",
       " (5, 1),\n",
       " (1, 1),\n",
       " (5, 1),\n",
       " (2, 2),\n",
       " (2, 2),\n",
       " (3, 2),\n",
       " (3, 2)]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums5.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "combineByKey函数主要接受了三个函数作为参数，分别为createCombiner、mergeValue、mergeCombiners总体效果为RDD[(K,V)]=>RDD[(K,C)],使用用户设置好的聚合 函数对每个key中的value进行组合函数原型为： def combinByKey[C](createCombiner:V=>C,mergeValue:(C,V)=>C,mergeCombiners:(C,C)=>C:RDD[(K,C)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 1.0), (1, 1.0), (5, 1.0), (2, 2.0), (3, 2.0)]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#combineByKey函数主要接受了三个函数作为参数，分别为createCombiner、mergeValue、mergeCombiners\n",
    "nums4 = nums5.combineByKey((lambda x:(x,1)),\n",
    "                             (lambda x,y:(x[0]+y,x[1]+1)),\n",
    "                             (lambda x,y:(x[0]+y[0],x[1]+y[1])))\n",
    "nums4.mapValues(lambda x:float(x[0]/x[1])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 1.0, 2: 2.0, 3: 2.0, 4: 1.0, 5: 1.0}"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums4.mapValues(lambda x:float(x[0]/x[1])).collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, (2, 2)), (1, (2, 2)), (5, (2, 2)), (2, (4, 2)), (3, (4, 2))]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums4.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((4, 1), 1),\n",
       " ((4, 1), 1),\n",
       " ((1, 1), 1),\n",
       " ((5, 1), 1),\n",
       " ((1, 1), 1),\n",
       " ((5, 1), 1),\n",
       " ((2, 2), 1),\n",
       " ((2, 2), 1),\n",
       " ((3, 2), 1),\n",
       " ((3, 2), 1)]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#下面来细分combineByKey函数的效果\n",
    "nums7=nums5.map(lambda x:(x,1))\n",
    "nums7.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', 4), ('a', 4)]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#在Python中自定义reduceByKey()的并行度\n",
    "data = [(\"a\",3),(\"b\",4),(\"a\",1)]\n",
    "sc.parallelize(data).reduceByKey(lambda x,y:x+y).collect()#默认并行度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', 4), ('a', 4)]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize(data).reduceByKey(lambda x,y:x+y,10).collect()#自定义并行度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#sortByKey\n",
    "#在Python中以字符串顺序对整数进行自定义排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#rdd=sc.parallelize([1,2,3,4])\n",
    "rdd=sc.parallelize(data)\n",
    "#rdd=rdd.map(lambda x:(x,1))\n",
    "rdd1=rdd.sortByKey(ascending = True,numPartitions = None,keyfunc = lambda x: str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 3), ('a', 1), ('b', 4)]"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面来介绍转换与行动：\n",
    "\n",
    "转换：转换的返回值是一个新的RDD集合，而不是单个值。调用一个变换方法，不会有任何求值计算，它只获取一个RDD作为参数，然后返回一个新的RDD。\n",
    "行动：行动操作计算并返回一个新的值。当在一个RDD对象上调用行动函数时，会在这一时刻计算全部的数据处理查询并返回结果值。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------\n",
    "Transformations:\n",
    "----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "combineByKey函数主要接受了三个函数作为参数，分别为createCombiner、mergeValue、mergeCombiners总体效果为RDD[(K,V)]=>RDD[(K,C)],使用用户设置好的聚合 函数对每个key中的value进行组合函数原型为： def combinByKey[C](createCombiner:V=>C,mergeValue:(C,V)=>C,mergeCombiners:(C,C)=>C:RDD[(K,C)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 1), (1, 1), (5, 1), (2, 2), (3, 2)]"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums1=sc.parallelize([1,2,3,4,5,3,2])\n",
    "nums2=nums1.map(lambda x:(x,1))\n",
    "nums3=nums2.reduceByKey(lambda x,y:x+y)\n",
    "nums5=nums3.union(nums3)\n",
    "nums3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 1),\n",
       " (4, 1),\n",
       " (1, 1),\n",
       " (5, 1),\n",
       " (1, 1),\n",
       " (5, 1),\n",
       " (2, 2),\n",
       " (2, 2),\n",
       " (3, 2),\n",
       " (3, 2)]"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums5.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 1.0), (1, 1.0), (5, 1.0), (2, 2.0), (3, 2.0)]"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#combineByKey函数主要接受了三个函数作为参数，分别为createCombiner、mergeValue、mergeCombiners\n",
    "nums4 = nums5.combineByKey((lambda x:(x,1)),\n",
    "                             (lambda x,y:(x[0]+y,x[1]+1)),\n",
    "                             (lambda x,y:(x[0]+y[0],x[1]+y[1])))\n",
    "nums4.mapValues(lambda x:float(x[0]/x[1])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, (2, 2)), (1, (2, 2)), (5, (2, 2)), (2, (4, 2)), (3, (4, 2))]"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums4.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter(func)  --- 返回一个新的数据集，这个数据集中的元素是通过func函数筛选后返回为true的元素（简单的说就是，对数据集中的每个元素进行筛选，如果符合条件则返回true，\n",
    "不符合返回false，最后将返回为true的元素组成新的数据集返回）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4]"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1=[1,2,3,4,5]\n",
    "rdd = sc.parallelize(data1).filter(lambda x:x%2==0)\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "flatMap(func [, preservesPartitioning=False])  --- 类似于map(func)， 但是不同的是map对每个元素处理完后返回与原数据集相同元素数量的数据集，\n",
    "而flatMap返回的元素数不一定和原数据集相同。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 2, 2, 3]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### for flatMap()\n",
    "rdd = sc.parallelize([2,3,4])\n",
    "sorted(rdd.flatMap(lambda x: range(1,x)).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 1, 2, 3]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([[2,3,4],[1,2,3]])\n",
    "rdd.flatMap(lambda x: x).collect() #是按最小元素来映射的，在这里是每个元素的映射，而不是两个列表的映射"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 2), (2, 2), (3, 3), (3, 3), (4, 4), (4, 4)]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(rdd.flatMap(lambda x:[(x,x), (x,x)]).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[range(1, 2), range(1, 3), range(1, 4)]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### for map()\n",
    "rdd = sc.parallelize([2,3,4])\n",
    "rdd.map(lambda x: range(1,x)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(2, 2), (2, 2)], [(3, 3), (3, 3)], [(4, 4), (4, 4)]]"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(rdd.map(lambda x:[(x,x), (x,x)]).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mapPartitions(func [, preservesPartitioning=False])  \n",
    "mapPartitions是map的一个变种。map的输入函数是应用于RDD中每个元素，而mapPartitions的输入函数是应用于每个分区，也就是把每个分区中的内容作为整体来处理的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 5, 9]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([1,2,3,4,5], 3)\n",
    "def f(iterator): yield sum(iterator) #分区求和\n",
    "rdd.mapPartitions(f).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2]"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mapPartitionsWithIndex生成分区的索引\n",
    "rdd = sc.parallelize([1,2,3,4,5], 3)\n",
    "def f(splitIndex, iterator): yield splitIndex\n",
    "rdd.mapPartitionsWithIndex(f).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reduceByKey(func [, numPartitions=None, partitionFunc=<function portable_hash at 0x7fa664f3cb90>]) \n",
    "reduceByKey就是对元素为kv对的RDD中Key相同的元素的value进行reduce，因此，key相同的多个元素的值被reduce为一个值，然后与原RDD中的key组成一个新的kv对。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 2), ('b', 1)]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import add\n",
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "sorted(rdd.reduceByKey(add).collect())\n",
    "#或者 sorted(rdd.reduceByKey(lambda a,b:a+b).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sortByKey([ascending=True, numPartitions=None, keyfunc=<function <lambda> at 0x7fa665048c80>])\n",
    "返回排序后的数据集。该函数就是队kv对的RDD数据进行排序，keyfunc是对key进行处理的函数，如非需要，不用管。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 3), ('D', 4), ('a', 1), ('b', 2)]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = [('a', 1), ('b', 2), ('1', 3), ('D', 4)]\n",
    "sc.parallelize(tmp).sortByKey(True, 1).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 3), ('a', 1), ('b', 2), ('D', 4)]"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize(tmp).sortByKey(True, 2, keyfunc=lambda k:k.lower()).collect()\n",
    "#注意，比较两个结果可看出，keyfunc对键的处理只是在数据处理的过程中起作用，不能真正的去改变键名"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "join(otherDataset [, numPartitions=None]) \n",
    "join就是对元素为kv对的RDD中key相同的value收集到一起组成(v1,v2)，然后与原RDD中的key组合成一个新的kv对，返回。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', (1, 2)), ('a', (1, 3))]"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
    "y = sc.parallelize([(\"a\", 2), (\"a\", 3)])\n",
    "sorted(x.join(y).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cartesian(otherDataset)  --- 返回一个笛卡尔积的数据集，这个数据集是通过计算两个RDDs得到的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 4), (1, 5), (2, 4), (2, 5), (3, 4), (3, 5)]"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = sc.parallelize([1,2,3])\n",
    "y = sc.parallelize([4,5])\n",
    "x.cartesian(y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sample(withReplacement, fraction, seed) \n",
    "从数据中抽样，withReplacement表示是否有放回，withReplacement=true表示有放回抽样，fraction为抽样的概率（0<=fraction<=1），seed为随机种子。 \n",
    "例如：从1-100之间抽取样本，被抽取为样本的概率为0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "！！！注意，Spark中的sample抽样，当withReplacement=True时，相当于采用的是泊松抽样；当withReplacement=False时，相当于采用伯努利抽样，fraction并不是表示抽样得到的样本占原来数据总量的百分比，而是一个元素被抽取为样本的概率。fraction=0.2并不是说明要抽出100个数字中20%的数据作为样本，而是每个数字被抽取为样本的概率为0.2，这些数字被认为来自同一总体，样本的大小并不是固定的，而是服从二项分布。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = sc.parallelize(range(1,101),2)\n",
    "sampleData = data.sample(True, 0.2)\n",
    "sampleData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7,\n",
       " 24,\n",
       " 25,\n",
       " 29,\n",
       " 33,\n",
       " 39,\n",
       " 42,\n",
       " 45,\n",
       " 46,\n",
       " 54,\n",
       " 60,\n",
       " 64,\n",
       " 69,\n",
       " 70,\n",
       " 79,\n",
       " 82,\n",
       " 83,\n",
       " 89,\n",
       " 91,\n",
       " 93,\n",
       " 95,\n",
       " 95,\n",
       " 97]"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampleData.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "intersection(otherDataset) \n",
    "交集操作，将源数据集与union中的输入数据集取交集，并返回新的数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 9, 6, 7]"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1 = sc.parallelize(range(10))\n",
    "data2 = sc.parallelize(range(6,15))\n",
    "data1.intersection(data2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "distinct([numTasks]) 去除数据集中的重复元素。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 8, 1, 9, 2, 10, 11, 3, 12, 4, 5, 13, 14, 6, 7]"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1 = sc.parallelize(range(10))\n",
    "data2 = sc.parallelize(range(6,15))\n",
    "data1.union(data2).distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------\n",
    "Action:\n",
    "--------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reduce(func)  --- reduce将RDD中元素两两传递给输入函数，同时产生一个新的值，新产生的值与RDD中下一个元素再被传递给输入函数直到最后只有一个值为止。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import add\n",
    "sc.parallelize([1,2,3,4,5]).reduce(add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "collect()  --- 返回RDD中的数据，以list形式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([1,2,3,4,5]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "count()  --- 返回RDD中的元素个数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([1,2,3,4,5]).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first()  --- 返回RDD中的第一个元素。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([1,2,3,4,5]).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "take(n)  --- 返回RDD中前n个元素。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([1,2,3,4,5]).take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "takeOrdered(n [, key=None])  --- 返回RDD中前n个元素，但是是升序(默认)排列后的前n个元素，或者是通过key函数指定后的RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4]"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.([9,7,3,2,6,4]).takeOrdered(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 7, 6]"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([9,7,3,2,6,4]).takeOrdered(3, key=lambda x:-x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "saveAsTextFile(path [, compressionCodecClass=None])  --- 该函数将RDD保存到文件系统里面，并且将其转换为文本行的文件中的每个元素调用 tostring 方法。\n",
    "parameters:  path - 保存于文件系统的路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NamedTemporaryFile' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-225-b164ab970718>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtempFile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNamedTemporaryFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtempFile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtempFile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mfileinput\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mglob\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'NamedTemporaryFile' is not defined"
     ]
    }
   ],
   "source": [
    "tempFile = NamedTemporaryFile(delete=True)\n",
    "tempFile.close()\n",
    "sc.parallelize(range(10)).saveAsTextFile(tempFile.name)\n",
    "from fileinput import input\n",
    "from glob import glob\n",
    "''.join(sorted(input(glob(tempFile.name + \"/part-0000*\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "countByKey()  --- 返回一个字典（key,count），该函数操作数据集为kv形式的数据，用于统计RDD中拥有相同key的元素个数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'a': 2, 'b': 1})"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "defdict = sc.parallelize([(\"a\",1), (\"b\",1), (\"a\", 1)]).countByKey()\n",
    "defdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('b', 1), ('a', 2)])"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "defdict.items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "countByValue()  --- 返回一个字典（value,count），该函数操作一个list数据集，用于统计RDD中拥有相同value的元素个数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([(1, 2), (2, 4), (3, 3), (5, 1)])"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([1,2,3,1,2,5,3,2,3,2]).countByValue().items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "foreach(func)  --- 运行函数func来处理RDD中的每个元素，这个函数常被用来updating an Accumulator或者与外部存储系统的交互。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f(x): print(x)\n",
    "sc.parallelize([1, 2, 3, 4, 5]).foreach(f)\n",
    "#note: 打印是随机的，并不是一定按1,2,3,4,5的顺序打印"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "collectAsMap:以rdd元素为元组，以元组中一个元素作为索引返回RDD中的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{3: 4, 'a': 2}\n"
     ]
    }
   ],
   "source": [
    "m = sc.parallelize([('a', 2), (3, 4)]).collectAsMap()\n",
    "print (m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "groupby函数：根据提供的方法为RDD分组："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, <pyspark.resultiterable.ResultIterable object at 0x000002418A4985C0>), (1, <pyspark.resultiterable.ResultIterable object at 0x000002418A4985F8>)]\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([1, 1, 2, 3, 5, 8])\n",
    "def fun(i):\n",
    "    return i % 2\n",
    "\n",
    "result=rdd.groupBy(fun).collect()\n",
    "print (result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, [2, 8]), (1, [1, 1, 3, 5])]\n"
     ]
    }
   ],
   "source": [
    "print ([(x, sorted(y)) for (x, y) in result])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据分区：数据比较大时，可以用partitionBy()转化为哈希分区。即通过向partitionBy传递一个spark.HashPartitioner对象来实现该操作。在Python中不能将HashPartitioner对象传递给partitionBy，只需要把需要的分区数传递过去（如 rdd.partitionBy(100)）。\n",
    "\n",
    "　　在spark中，会为生成的结果RDD设好分区方式的操作有：cogroup(),groupWith(),join(),leftOuterJoin(),rightOutJoin,groupByKey(),reduceByKey(),combineByKey(),partitionBy(),sort(),mapValues(),flatMapValues(),filter()。最后三种只有当父RDD有分区方式时，结果RDD才会有分区RDD。其他的操作生成的结果都不会存在特定的分区方式。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a=[1,2,3,2]\n",
    "a.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1, 2), (2, 3), (3, 4), (0, 9)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1=[(1,2),(2,3),(0,9),(3,4)]\n",
    "print (type (a1))\n",
    "sorted(a1,key=lambda x:x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a=np.zeros(2)\n",
    "print (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
